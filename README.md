# Gradient Descent in Gory Detail: OLS to Mini-batching and Beyond

This work is a Gradient Descent walkthrough for the Technology and AI Learning Seminar (TAILS) at Montana State University on 10/17/2025 presented by [@swhammon](https://github.com/swhammon) and myself. 

The abstract:

Gradient descent is central to machine learning, yet few people have actually carried out its steps by hand. This presentation offers a detailed, hands-on walkthrough of how the algorithm works in practice. Beginning with the closed-form ordinary least squares (OLS) solution for linear regression for comparison, weâ€™ll then derive the gradient, implement updates, and compare batch, stochastic, and mini-batch approaches. Attendees will leave with a grounded, intuitive understanding of how gradient descent optimization unfolds step by step.
